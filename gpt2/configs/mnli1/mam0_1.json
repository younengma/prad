{
  "model_name_or_path": "/data/myn/llm/kernel-adapters-main/nlg/pretrained/gpt2",
  "output_dir": "output",
  "cache_dir": "hf_cache",
  "train_file": "data/en_ro/train.json",
  "validation_file": "data/en_ro/validation.json",
  "test_file": "data/en_ro/test.json",
  "ignore_pad_token_for_loss": true,
  "do_train": true,
  "do_eval": true,
  "do_predict": true,
  "lr_scheduler_type": "linear",
  "eval_checkpoint": null,
  "task_name": "mnli1",
  "source_column": "source",
  "target_column": "target",
  "report_to": "tensorboard",
  "evaluation_strategy": "steps",
  "save_strategy": "steps",
  "eval_steps": 3000,
  "logging_steps": 200,
  "save_steps": 3000,
  "max_tokens_per_batch": 0,
  "max_train_samples": null,
  "max_steps": -1,
  "num_train_epochs": 5,
  "max_eval_samples": 4000,
  "max_predict_samples":null,
  "per_device_train_batch_size": 32,
  "per_device_eval_batch_size": 32,
  "gradient_accumulation_steps": 1,
  "learning_rate": 2e-3,
  "warmup_ratio": 0.06,
  "save_total_limit": 2,
  "load_best_model_at_end": true,
  "save_safetensors": false,
  "seed": 42,
  "data_seed": 42,
  "bf16": true,
  "overwrite_output_dir": true,
  "preprocessing_num_workers": 4,
  "max_source_length": 128,
  "max_target_length": 24,
  "val_max_target_length": 24,
  "num_beams": 1,
  "no_repeat_ngram_size": 0,
  "predict_with_generate": true,

  "attn_mode":"prefix",
  "attn_option":"parallel",
  "attn_r":54,
  "attn_adapter_scalar":1,

  "ffn_mode":"adapter",
  "ffn_option":"parallel",
  "ffn_adapter_layernorm_option":"none",
  "ffn_adapter_scalar":1,
  "ffn_r":16,

  "lora_alpha":32,
  "lora_dropout":0.1,

  "apply_prompt_tuning":false,
  "prompt_length":30,
  "prefix_length":32,
  "apply_c_masks":false
}
